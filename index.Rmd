---
title: "80's New Romantics vs 90's Trance: An Analytical Comparison"
author: "Joy Crosbie"
date: "Spring 2021"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme: lumen
    orientation: rows
    vertical_layout: fill
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(tidymodels)
library(ggdendro)
library(heatmaply)
library(spotifyr)
library(plotly)
library(kableExtra)
library(compmus)
library(patchwork)

get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit %>% 
    collect_predictions() %>% 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit %>% 
    conf_mat_resampled() %>% 
    group_by(Prediction) %>% mutate(precision = Freq / sum(Freq)) %>% 
    group_by(Truth) %>% mutate(recall = Freq / sum(Freq)) %>% 
    ungroup() %>% filter(Prediction == Truth) %>% 
    select(class = Prediction, precision, recall)
}  

eighties <- get_playlist_audio_features("", "7hIxxRmPxdpBa6zuDUC1FM")
trance <- get_playlist_audio_features("", "3XKJmcxhdP8NXIoamf5zwc")
```


```{r create playlists}
playlists <-
  bind_rows(
    eighties %>% mutate(playlist = "New Romantic"),
    trance %>% mutate(playlist = "Trance")
  )
```




### Classification

```{r create features(slow!)}

  playlists_features <-
  playlists %>%  # For your portfolio, change this to the name of your corpus.
  add_audio_analysis() %>% 
  mutate(
    playlist = factor(playlist),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) %>%
  mutate(pitches = map(pitches, compmus_normalise, "clr")) %>%
  mutate_at(vars(pitches, timbre), map, bind_rows) %>%
  unnest(cols = c(pitches, timbre))
```

```{r}
playlists_recipe <-
  recipe(
    playlist ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = playlists_features,          # Use the same name as the previous block.
  ) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())      # Converts to z-scores.
  # step_range(all_predictors())    # Sets range to [0, 1].

playlists_cv <- playlists_features %>% vfold_cv(5)
```

```{r}
forest_model <-
  rand_forest() %>%
  set_mode("classification") %>% 
  set_engine("ranger", importance = "impurity")
playlists_forest <- 
  workflow() %>% 
  add_recipe(playlists_recipe) %>% 
  add_model(forest_model) %>% 
  fit_resamples(
    playlists_cv, 
    control = control_resamples(save_pred = TRUE)
  )
```
```{r}
forest_pr <- playlists_forest %>% get_pr()
#playlists_forest %>% get_conf_mat() %>% autoplot(type = "heatmap")
```

```{r eval=FALSE}
workflow() %>% 
  add_recipe(playlists_recipe) %>% 
  add_model(forest_model) %>% 
  fit(playlists_features) %>% 
  pluck("fit", "fit", "fit") %>%
  ranger::importance() %>% 
  enframe() %>% 
  mutate(name = fct_reorder(name, value)) %>% 
  ggplot(aes(name, value)) + 
  geom_col() + 
  coord_flip() +
  theme_minimal() +
  labs(x = NULL, y = "Importance")

```

```{r}
playlists_recipe_reduced <-
  recipe(
    playlist ~
      instrumentalness +
      valence +
      tempo +
      c03 + c06,
    data = playlists_features,          # Use the same name as the previous block.
  ) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())      # Converts to z-scores.
  # step_range(all_predictors())    # Sets range to [0, 1].
playlists_forest_reduced <- 
  workflow() %>% 
  add_recipe(playlists_recipe) %>% 
  add_model(forest_model) %>% 
  fit_resamples(
    playlists_cv, 
    control = control_resamples(save_pred = TRUE)
  )
```

```{r}
forest_reduced_pr <- playlists_forest_reduced %>% get_pr()
#playlists_forest_reduced %>% get_conf_mat() %>% autoplot(type = "heatmap")
```



```{r}
decision <- playlists_features %>%
  ggplot(aes(x = c06, y = valence, colour = playlist, size = valence)) +
  geom_point(alpha = 0.8) +
  scale_color_viridis_d() +
  labs(
    x = "c06",
    y = "Tempo",
    size = "Valence",
    colour = "Playlist"
  )

```

```{r}
svm_model <-
  svm_rbf() %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")
playlists_svm <- 
  workflow() %>% 
  add_recipe(playlists_recipe_reduced) %>% 
  add_model(svm_model) %>% 
  fit_resamples(
    playlists_cv, 
    control = control_resamples(save_pred = TRUE)
  )
  
```

```{r}

svm_pr <- playlists_svm %>% get_pr()
confusematrix <- playlists_svm %>% get_conf_mat() %>% autoplot(type = "heatmap")

decision + confusematrix
```

***
The first classifier I tried when trying to distinguish between the trance and New Romantic tracks was a random forest classifier. This classifier already performs well, as shown in the following table: 
```{r}
forest_pr %>%
  kbl() %>%
  kable_styling()
```
Using a random-forest classifier, the most important features for classifying tracks among these playlists are (ranked from most to least important):
1. Timbre Component 6
2. Valence
3. Timbre Component 3
4. Tempo
5. Instrumentalness
After these five features, there is a substantial drop in importance. The results of the random forest classifier with just these five features are shown below:

```{r}
forest_reduced_pr %>%
  kbl() %>%
  kable_styling()
```

The classifier achieves a higher precision and recall for both genres when only the top 5 most important features are used.

When plotting the two most important features, a clear non linear decision boundary can be observed. This lead me to try a Support Vector Machine (SVM), in the hopes to be able to capture the corresponding decision regions. The SVM results are shown below:

```{r}
svm_pr %>%
  kbl() %>%
  kable_styling()
```
While the precision stays the same, the recall goes up by 3/4% . Although it follows from the scores that a few trance tracks were thought to be New Romantic tracks, the SVM does a very good job at distinguishing between the two.


### Introduction
My corpus consists of a playlist containing 80's New Romantic songs and a playlist containing trance classics from the early 90's to the mid 2010's. I was inspired by a statement made by BBC Radio 2, referring to Blue Monday (New Order's 1983 hit), as "a crucial link between Seventies disco and the dance/house boom that took off at the end of the New Romantic." The repetitive use of synths and heavy drum beats is a known characteristic of both new wave and trance music, and lead me to compare the two and see if any links could indeed be found.

Although the synthesizer is common in both playlists, there are some obvious differences when it comes to the tones. The 'punchy' synth sounds from the 80's have become more complex chord progressions of sweeping strings in trance music. I believe this will also be apparent in the valence and keys of the compared genres, because of the dark and "poppy" influences of the new-romantic style compared to the euphoric feeling of trance music. Interestingly, the number of female voices in the trance songs is significantly larger than the number of male voices, which is less apparent in the 80's genres. Moreover, the number of instrumental tracks is notably larger in the trance playlist. I am curious to see the difference in the 'energy' measured by spotify, as I believe both genres contain a high amount of energy, but the energy they contain is different, as described above.

Typical examples of interesting tracks from the 80's playlist would be Blue Monday - New Order and Fade To Grey - Visage. Both tracks contain a basic chord progression in a minor key played on a synthesizer repeated throughout the song, accompanied by a similar drum and high hat beat. Typical examples of interesting tracks from the trance playlist would be Adagio for Strings - TiÃ«sto and Carte Blanche - Veracocha. Both songs contain well known riffs and are comprised of a musical form that distinctly builds tension to either end in "peaks" or "drops". Both songs are also in minor keys, a common characteristic of trance music.


[New Romantic Playlist](https://open.spotify.com/playlist/7hIxxRmPxdpBa6zuDUC1FM?si=TxVg34i0TZSDGWSvYl7_Tg)

[Trance Playlist](https://open.spotify.com/playlist/3XKJmcxhdP8NXIoamf5zwc?si=svUOmdo4RRy29MBrfyWc6A)

### Club comeback: Which is more danceable, Trance or 80's New Romantics?
```{r}
dancetempo <- playlists %>%                    # Start with awards.
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
  ) %>%
  ggplot(                     # Set up the plot.
    aes(
      x = tempo,
      y = danceability,
      name = track.name,
      color = playlist
    )
  ) +
  geom_point(show.legend = FALSE) + 
  # Scatter plot.
  geom_rug(size = 0.1) +      # Add 'fringes' to show data distribution. 
  facet_wrap(~playlist) +     # Separate charts per playlist.
  scale_x_continuous(         # Fine-tune the x axis.
    limits = c(105, 170),
    #breaks = c(0, 0.50, 1),   # Use grid-lines for quadrants only.
    minor_breaks = NULL       # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(         # Fine-tune the y axis in the same way.
    limits = c(0.3, 1),
    breaks = c(0.5, 0.75, 1),
    minor_breaks = NULL
  ) +
  scale_color_discrete(guide = 'none') +
  theme_light() +             # Use a simpler theme.
  labs(                       # Make the titles nice.
    x = "Tempo(BPM)",
    y = "Danceability"
  ) +
  ggtitle('Danceability: New Romantic vs. Trance') +
  theme(plot.title = element_text(hjust = 0.5))

dancetempo <- dancetempo + theme(legend.position = "none")

ggplotly(dancetempo)
```

***
An immediate first observation is that the trance tracks range over a much smaller BPM interval.This was expected, as the genre is characterized by a tempo lying between 135-150 BPM. 

The majority of the tracks in both genres seem to lie between the 0.5-0.75 danceability interval. Worth noting however, is the amount of trance tracks with a 'low' danceability under 0.5. 

### Trance: not very danceable dance music? 
```{r}
sunrise <- get_tidy_audio_analysis("3dnGed4dwCkO2AoQVnbScw")
```

```{r}
sunrise%>%
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  ggtitle('Tempogram of Sunrise - Ratty') +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic() 
```

*** 
**This comes after the danceability/tempo plot** 
As shown in the previous plot, quite a few of the trance tracks have been assigned a low danceability value. A common characteristic of trance music is a mid-song climax followed by a soft breakdown disposing of beats and percussion entirely, leaving the melody or atmospherics to stand alone for an extended period before gradually building up again. 

In [Sunrise (Radio-Edit) - Ratty](https://open.spotify.com/track/3dnGed4dwCkO2AoQVnbScw?si=F6ZgC9z6TbW757Am-VSEdw) these breakdown sections consist of female vocals in a style reminiscent of an elegy, with soft synths playing in the background. The lack of percussion causes the Spotify API to have trouble determining the tempo, as can be seen between 55 and 105 seconds and again after 200 seconds. This is presumably on of the reasons of the low danceability score (0.46), as danceable songs are presumed to have a relatively stable rhythm. Other songs with a low danceability score such as [Communication](https://open.spotify.com/track/0W0tn4jpmnwnPZxO8nyhS7?si=i_NuEaQgTuWn4In4kTvNHQ) and [Shivers](https://open.spotify.com/track/0pfJH3eRkM4t9wlg6LwuTU?si=DoPEjtEMRTqm588RWUNMjQ) also contain acapella breakdowns.

### The most danceable trance track: A closer look at the tempo

```{r}
doom <- get_tidy_audio_analysis("12GwtT1n24TIpp2RER56Ij")
```

```{r}
doom %>%
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  ggtitle('Tempogram of Dooms Night (Timo Maas Radio Edit) - Azzido Da Bass') +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```

***

[Dooms Night - Timo Maas Radio Edit by Azzido Da Bass](https://open.spotify.com/track/12GwtT1n24TIpp2RER56Ij?si=7IeAZT3NRnCWKbxQBlYwNQ) received the highest danceability score of 0.91. This tempogram is quite interesting. Once again the characteristic sections of trance music are visible. The song starts off with just percussion at 135BPM. At 30 seconds the intro ends and the percussion stops and the brakdown starts, a synthesizer makes a 'womping' noise that speeds up slightly to 90 BPM. At 50 seconds the percussion joins the synth and takes the tempo back up to 135 BPM up until 95 seconds. This is where it gets interesting: again the percussion stops and the "womping" synth starts off slowly and then builds up to 300BPM. The first half of the song is then repeated again. **I presume the 300 BPM isn't shown because it is wrapped and in the graph it becomes the second sloped line. How do I solve this? Or can I just leave it like this if I just explain it @Tosca?**

### The most danceable New Romantics track: A closer look at the tempo

```{r}
electricavenue <- get_tidy_audio_analysis("10kYRZ5byZgBEcmmQizlzj")
```

```{r}
electricavenue %>%
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  ggtitle('Tempogram of Electric Avenue - Eddy Grant') +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```

***

[Electric Avenue - Eddy Grant](https://open.spotify.com/track/10kYRZ5byZgBEcmmQizlzj?si=egL6bvrHS3CO1nc6s2HGkQ) received a very high danceability score of 0.95. The structure of this song is very different to the trance tracks, as it doesn't consist of sections with a different tempo (this is true for most of the New Romantic tracks). In fact, the song is very repetitive with the same drums, synth melody and bass riff playing throughout the entire track. The whole song has a BPM around 121 BPM.

### Is trance music really happier than the 80's New Romantic music?
```{r}
scatter_feeling <- playlists %>%                    # Start with awards.
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
  ) %>%
  ggplot(                     # Set up the plot.
    aes(
      x = valence,
      y = energy,
      size = loudness,
      colour = mode,
      name = track.name
    )
  ) +
  geom_point(alpha = 0.45, width = 0.1) +
  #geom_jitter(alpha = 0.45, width = 0.1) +
  facet_wrap(~ playlist) +
   scale_x_continuous(         # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),   # Use grid-lines for quadrants only.
    minor_breaks = NULL       # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(         # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
   scale_size_continuous(      # Fine-tune the sizes of each point.
    trans = "exp",            # Use an exp transformation to emphasize loud.
    guide = "none"            # Remove the legend for size.
  ) +
  theme_light() +             # Use a simpler theme.
  labs(                       # Make the titles nice.
    x = "Valence",
    y = "Energy",
    colour = "Mode"
  ) + ggtitle('Energy vs. Valence: Overall emotion of tracks') +
  theme(plot.title = element_text(hjust = 0.5))
ggplotly(scatter_feeling)
```

***
As shown in the plot above, both the New Romantic and the trance tracks contain a high amount of energy. Surprisingly, the trance tracks generally emit a lower range of valence. High energy and low valence would indicate that trance music would be classified as angry. This is in contrast to the euphoric or slightly sad feeling the heavy synths are generally described to have. Mostly high energy and high valence means the New Romantic tracks would generally be perceived to be happy, though there are some in both the sad and angry region. 

The trace tracks are also louder than the New Romantic ones with an average loudness of -9.9 compared to -8. 

Finally the amount of tracks in both major and minor keys seems to be about the same. I had expected more minor tracks in the trance playlist, but this was purely based on minor tracks being a common characteristic of trance music.

### A (rare) 'euphoric' trance track: Peaks and vocals.
```{r}
ernesto <-
  get_tidy_audio_analysis("0pMUR7Uvp6vxlbG0qBFvgM") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

ernesto %>%
  mutate(pitches = map(pitches, compmus_normalise, "manhattan")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "Dark Side Of The Moon - Ernesto vs. Bastian") +
  theme_minimal() +
  scale_fill_viridis_c()
```

***
[Dark Side Of The Moon by Ernesto vs. Bastian](https://open.spotify.com/track/05lLA8ljMPozdEtFycI52o?si=_t7JeD6QQAK5jIO3w0vMsw) seems to stand out from the majority of 'angry' dance tracks and contains the highest valence in the trance dataset. Here the corresponding chromagram is shown. 

The song is a typical example of the structure of a trance track, as it is comprised of sections that distinctly build tension to either end in "peaks" or "drops". Here those sections can clearly be seen by the absence of the vocals which mostly make up B and C. 

The sequence on the synths (D#/E, F# and G#) repeats throughout the song and becomes louder when the vocals stop. At 120 sec there is a pure synth buildup to a 'peak' which ends around 150 seconds.

### Possible explanation for the angry trance tracks? - A comparison of musical keys for New Romantic and Trance tracks
```{r}
playlists$key[playlists$key == "0"] <- "C"
playlists$key[playlists$key == "1"] <- "C#"
playlists$key[playlists$key == "2"] <- "D"
playlists$key[playlists$key == "3"] <- "D#"
playlists$key[playlists$key == "4"] <- "E"
playlists$key[playlists$key == "5"] <- "F"
playlists$key[playlists$key == "6"] <- "F#"
playlists$key[playlists$key == "7"] <- "G"
playlists$key[playlists$key == "8"] <- "G#"
playlists$key[playlists$key == "9"] <- "A"
playlists$key[playlists$key == "10"] <- "A#"
playlists$key[playlists$key == "11"] <- "B"

playlists <- playlists %>%
  mutate(Key = if_else(mode == 0, paste(key, 'm', sep = ""), key))
```

```{r}
playlists1 <- playlists
playlists1$Key <- factor(playlists1$Key, levels = c("A", "Am", "A#", "A#m", "B", "Bm","C", "Cm", "C#", "C#m", "D", "Dm", "D#", "D#m", "E", "Em", "F", "Fm", "F#", "F#m", "G", "Gm", "G#", "G#m"))
ggplot(playlists1, aes(x = Key, fill = playlist)) + 
  geom_bar(position = 'dodge') +
  ggtitle('Keys of New Romantic and Trance tracks') +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1))
```

***

**New Romantic analysis**

On the left a bar chart of the keys of the tracks for both genres is shown. The first clear observation is the large amount of New Romantic tracks in the key of C major. This key is associated with a feeling of almost childlike innocence and simplicity. A typical example in this category would be [Just can't get enough by Depeche mode](https://open.spotify.com/track/0qi4b1l0eT3jpzeNHeFXDT?si=dbmWtCbgR2ejrj9GVcLTOg), because of it's cheerful and imaginative synth melodies. 

Both A and A# also contain a relatively high amount of New Romantic tracks. A is perceived as joyful and gives the sense of an declaration of love. An example in this key would be [Just Like Heaven by The Cure](https://open.spotify.com/track/4NnWuGQujzWUEg0uZokO5M?si=3jG8sicxRBWgrLNZkPIZXg), for it's youthful cheerfulness. A# is closely related with a more optimistic and future orientated feeling. 

Finally, F only seems to contain New Romantic tracks. This key is said to give off a sense of deep anger that is controlled, or a sense of regret. [Regret by New Order](https://open.spotify.com/track/1UVll7jjUlqSGxCZ6uXVaD?si=-HhVlCL5SXmgWEiBlL67xA) (how fitting) is an  example of a track in F. 

**Trance analysis**

The first clear observation is the large amount of trance tracks in the key of G#. This key is perceived as a sense of death, eternity and judgment. At first the large amount of tracks seemed strange to me, but I believe this key gives off the haunting feeling that many trance tracks have. I read that this key could be described as "Expansive viewpoints of a dark cosmos and existence" and I feel this is indeed so. [Universal Nation by Push](https://open.spotify.com/track/34PgbZHudjUapNEqsb1WcW?si=xwvX8fEtQnOQtKywH00NRA) is a song in G# that really feels like a trip through the emptiness and eternity of space.

This trend of negative feelings continues in the large amount of songs in the keys of Gm, C# and A#m. All keys have a sense of discontent, ranging from uneasiness in Gm, to a sense of grief and depression in C# to simply 'terrible' and giving up in A#m. 

However the relatively high songs in G contain a feeling of fantasy and peace, which is more what I expected to find. As the rush comes - Motorcycle contains a lot of dreamy background choir vocals and emits a strong feeling of peace. The tender and graceful key of Am also contains quite a few songs, such as [9pm (Till I come) - ATB](https://open.spotify.com/track/1CgbwsrNDlFrRuk2ebQ7zr?si=crFultzrRQSjVb0_2CNt3Q).

**Conclusion** 
Looking through the tracks in each key, the characteristics of each key seem to clearly be reflected in the general feel of the track. A large amount of the trance tracks are in a key that is associated with negative emotions. These negative emotions, combined with the high amount of energy appears to be a plausible explanation for the large amount of tracks classified as "angry". Interestingly, the spread of the tracks in the "overall emotions" plot seems to be reflected in the key plot.

```{r include = FALSE}
depechemode <-
  get_tidy_audio_analysis("6rvXznPVt21K7Adhw9zKEJ") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

depechemode %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "FTG") +
  theme_minimal() +
  scale_fill_viridis_c()
```

### Chroma and timbre in 80's New Romantic pop: A typical example

```{r}
duel <-
  get_tidy_audio_analysis("4M2tEAdFRdg0U8rAgYa8o2") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"
      )
  )
bind_rows(
  duel %>%
    compmus_self_similarity(pitches, "manhattan") %>%
    mutate(d = d / max(d), type = "Chroma"),
  duel %>%
    compmus_self_similarity(timbre, "euclidean") %>%
    mutate(d = d / max(d), type = "Timbre")
) %>%
  mutate() %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "") +
  ggtitle('Comparison of timbre and chroma features: Duel - Propaganda') +
  theme(plot.title = element_text(hjust = 0.5))
```

*** 
[Duel by Propaganda](https://open.spotify.com/track/4M2tEAdFRdg0U8rAgYa8o2?si=NqMAbydvRBK9_zgHDP6v1w) is a typical example of an 80's New Romantic track. The track contains two basic synth chord progressions: one that is repeated throughout the verses and one for the chorus. During the pre-chorus a single chord is played. These repetitive sections can clearly be seen in the chroma plot, with the first verse lasting from 10-47 seconds, followed by the pre-chorus from 47-60 seconds and then the chorus from 60-90 seconds. The second chorus is followed by an instrumental bridge as can be seen by the distinctive section ranging from 160-200 seconds. The chorus is then repeated until the end of the song.

An important observation with this track is that the chroma plot and the timbre plot seem to show the same similarity pattern. This is because Both plots show the distinctive sections and their repetitions, as there is no variation between the different verses. The bridge is particularly interesting here, as the timbre is different to the other sections of the song. The song suddenly slows down and the melody changes for about ten seconds, to then instrumentally build up to the chorus. 

### Chroma and timbre in 90's trance: A typical example

```{r}
children <-
  get_tidy_audio_analysis("73iNrs3ww2JCPHS4pgTryg") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  children %>%
    compmus_self_similarity(pitches, "aitchison") %>%
    mutate(d = d / max(d), type = "Chroma"),
  children %>%
    compmus_self_similarity(timbre, "euclidean") %>%
    mutate(d = d / max(d), type = "Timbre")
) %>%
  mutate() %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")  +
  ggtitle('Comparison of timbre and chroma features: Children - Robert Miles') +
  theme(plot.title = element_text(hjust = 0.5))
```

***
[Children by Robert Miles](https://open.spotify.com/track/73iNrs3ww2JCPHS4pgTryg?si=hz1Mn-qtRpKItDDrNY-tIQ) is a well known trance classic from 1995. Many are familiar with its dream-like melodies and steady four-on-the-flour bass drum. Miles was inspired both by a response to photographs of child Yugoslav war victims that his father had brought home from a humanitarian mission and his idea to create a track to end DJ sets, intended to calm rave attendants prior to their driving home as a means to reduce car accident deaths. 

The chroma plot shows the repetition of two main sections, with the checkerboard pattern from 45-95 seconds being the chorus with the well known dreamy synth melody. The section from 95-135 seconds contains a heavy beat with shorter staccato synth melodies.

The timbre plot is interesting, as it doesn't correspond to the clear checkerboard pattern shown in the chroma plot. This is quite strange, as the simple structure of the song should be reflected here too. At 190 seconds the timbre of the song is different from other sections, as the song melodies pick up in speed and become more urgent than dreamy. 


